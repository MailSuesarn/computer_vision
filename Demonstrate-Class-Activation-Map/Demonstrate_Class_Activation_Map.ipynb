{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demonstrate_Class_Activation_Map",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPhKr9dT+SFeRloRwJ2vG5m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MailSuesarn/MailSuesarn.github.io/blob/master/Demonstrate_Class_Activation_Map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyrUk78Z8daU"
      },
      "source": [
        "📁 *This dataset is part of the MVTec dataset* 📁\r\n",
        "\r\n",
        "Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger. MVTec AD - A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection; in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019\r\n",
        "\r\n",
        "[MVTec Dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)\r\n",
        "\r\n",
        "This Colab is a part of the Super AI Engineer article\r\n",
        "> Author: Suesarn Wilainuch (22p21c0153)\r\n",
        "\r\n",
        "> Home: EXP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70ykGy-r_4y9"
      },
      "source": [
        "🔴🟢🔵 let's get started! 🔴🟢🔵"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdoDMMhhdN7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192044b7-6e4c-4456-efa5-a012944cfc7f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/gdrive')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcSpyKpk0ax8"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.utils import Progbar\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.applications import EfficientNetB3\r\n",
        "\r\n",
        "import os\r\n",
        "from tqdm import tqdm\r\n",
        "from numpy import load\r\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZO4iFV0ODm"
      },
      "source": [
        "# Load dataset (in memory (RAM))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoatbZ72dN-F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83821b31-2299-4d9f-f01a-4c135187303d"
      },
      "source": [
        "def load_data(filename):\r\n",
        "    data = load(filename)\r\n",
        "    X, y = data['arr_0'], data['arr_1']\r\n",
        "    return X, y\r\n",
        "\r\n",
        "# dataset path\r\n",
        "X, y = load_data('/gdrive/My Drive/Tensorflow2_tutorial/Medium&Video/hazelnut_dataset.npz')\r\n",
        "\r\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(170, 300, 300, 3) (170,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiGRYn6E0uqX"
      },
      "source": [
        "# Split train, test and prepare training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en6kWGNMdOC3"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=153, stratify=y)\r\n",
        "\r\n",
        "trainset = tf.data.Dataset.from_tensor_slices((X_train.astype('float32'), y_train)).shuffle(len(X_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D2a-ayT1Mye"
      },
      "source": [
        "# Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD73QNXhdOM8"
      },
      "source": [
        "def create_model(image_shape):\r\n",
        "\r\n",
        "    in_image = tf.keras.layers.Input(shape=image_shape)\r\n",
        "\r\n",
        "    model = EfficientNetB3(include_top=False, input_tensor=in_image, weights=\"imagenet\")\r\n",
        "\r\n",
        "    model.trainable = True\r\n",
        "\r\n",
        "    f = tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\r\n",
        "    top_dropout_rate = 0.4\r\n",
        "    f = tf.keras.layers.Dropout(top_dropout_rate, name=\"top_dropout\")(f)\r\n",
        "    outputs = tf.keras.layers.Dense(2, activation=\"softmax\", name=\"pred\")(f) # change activation and number of output node \r\n",
        "\r\n",
        "    # Compile\r\n",
        "    model = tf.keras.Model(in_image, outputs)\r\n",
        "    opt = tf.keras.optimizers.Adam(lr=0.0001)  \r\n",
        "    model.compile(loss=['sparse_categorical_crossentropy'], optimizer=opt, metrics=['accuracy']) # change loss function\r\n",
        "    model.summary()\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m0OUqO9_-Qp"
      },
      "source": [
        "IMAGE_SHAPE = (300, 300, 3)\r\n",
        "model = create_model(IMAGE_SHAPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDJ_4T-A1U1X"
      },
      "source": [
        "# function for create CAM (select activation function softmax or sigmoid) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUk8m_b-WoP"
      },
      "source": [
        "def create_CAM(input_image, model, activation='softmax'):\r\n",
        "    CAM_list = list()\r\n",
        "    n = input_image.shape[0]\r\n",
        "\r\n",
        "    last_conv = tf.keras.Model(model.input, model.get_layer(\"top_activation\").output) # specified by layer name\r\n",
        "    weights = model.layers[-1].get_weights()[0]  # weight at output class\r\n",
        "    feature_map = last_conv.predict(input_image) # (batch, h, w, c)\r\n",
        "    pred_prob = model.predict(input_image)\r\n",
        "    pred = np.squeeze(np.argmax(pred_prob, axis=-1))\r\n",
        "\r\n",
        "    ch = feature_map.shape[-1] # channel\r\n",
        "    hw = feature_map.shape[1]  # height = width\r\n",
        "    N = hw * hw  # N = height x width\r\n",
        "    feature_map = feature_map.reshape(-1, N, ch) # (bs, h, w, c) --> (bs, N, c)\r\n",
        "\r\n",
        "    for i in tqdm(range(n)):\r\n",
        "      if activation == 'sigmoid':\r\n",
        "        weights_class = weights[:, 0] # weight of activate class in this case we have only one (sigmoid)\r\n",
        "      else:\r\n",
        "        # pred[i] = 1 - pred[i] # toggle for plot deactivate class\r\n",
        "        weights_class = weights[:, pred[i]] # select weight that correspond to activate class \r\n",
        "\r\n",
        "      feature = feature_map[i]\r\n",
        "      CAM = np.dot(feature, weights_class).reshape((hw, hw, 1))  # (h, w, 1)\r\n",
        "      CAM_list.append(CAM)\r\n",
        "  \r\n",
        "    return np.asarray(CAM_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5jsRh80Cp3k"
      },
      "source": [
        "# Plot CAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-oeC606CqCf"
      },
      "source": [
        "from IPython import display\r\n",
        "\r\n",
        "def plot_CAM(input_image, CAMs, pred):\r\n",
        "  rows = 4\r\n",
        "  cols = 4\r\n",
        "  axes = []\r\n",
        "  fig = plt.figure()\r\n",
        "\r\n",
        "  display.clear_output(wait=True)\r\n",
        "  for i in range(rows*cols):\r\n",
        "      axes.append(fig.add_subplot(rows, cols, i+1))\r\n",
        "      # label\r\n",
        "      label = \"Good\" if y_test[i] == 0 else \"Bad\" \r\n",
        "\r\n",
        "      # predicted\r\n",
        "      class_name = \"Good\" if pred[i] == 0 else \"Bad\" \r\n",
        "\r\n",
        "      if class_name == label:\r\n",
        "        subplot_title=(f\"{class_name}|{label}\")\r\n",
        "        axes[-1].set_title(subplot_title, color='k')\r\n",
        "      else:\r\n",
        "        subplot_title=(f\"{class_name}|{label}\")\r\n",
        "        axes[-1].set_title(subplot_title, color='r')\r\n",
        "\r\n",
        "      plt.axis('off')  \r\n",
        "      plt.imshow(input_image[i].astype('uint8'))\r\n",
        "      CAM = CAMs[i]\r\n",
        "      CAM = cv2.resize(CAM, (X.shape[1], X.shape[1]))\r\n",
        "      plt.imshow(CAM, cmap='jet', alpha=0.3)\r\n",
        "  fig.tight_layout()    \r\n",
        "  plt.savefig(str(epoch+1) + '.png')\r\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGHqFU0M2g2s"
      },
      "source": [
        "# Training & create CAM on each epoch (Use train_on_batch method)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP2xvDVF2fc-"
      },
      "source": [
        "batch_size = 16\r\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H60tuFeOzean"
      },
      "source": [
        "for epoch in tqdm(range(epochs)):\r\n",
        "\r\n",
        "    # Loop on each batch of training\r\n",
        "    for idX, (X_batch, y_batch) in enumerate(trainset.batch(batch_size)):\r\n",
        "        \r\n",
        "      loss, acc = model.train_on_batch(X_batch, y_batch)\r\n",
        "        \r\n",
        "    print(f\"loss: {loss}, acc: {acc}\")\r\n",
        "\r\n",
        "    # plot CAM and image for visualization on each epoch\r\n",
        "    CAMs = create_CAM(X_test, model, activation='sigmoid') \r\n",
        "\r\n",
        "    pred_prob = model.predict(X_test)\r\n",
        "\r\n",
        "    pred = np.argmax(pred_prob, axis=-1) # for softmax \r\n",
        "    # pred = np.round(pred_prob, 1).astype(np.int32) # for sigmoid\r\n",
        "\r\n",
        "    plot_CAM(X_test, CAMs, pred)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJFRIEix6jxz"
      },
      "source": [
        "# Training & create CAM on each epoch (Use custom training loop method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tm0Rk6c6sRq"
      },
      "source": [
        "Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJdEfs3E6pQy"
      },
      "source": [
        "sparse_cat_cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "\r\n",
        "def loss_fn(y_true, y_pred):\r\n",
        "    loss = sparse_cat_cross_entropy(y_true, y_pred)\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H71Gi_2e65V2"
      },
      "source": [
        "Define the optimizer and prepare the metrics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGmTJh0M62CD"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)\r\n",
        "\r\n",
        "acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\r\n",
        "\r\n",
        "metrics_names = ['train_loss', 'train_acc']\r\n",
        "num_training_samples = X_train.shape[0]\r\n",
        "epochs = 100\r\n",
        "batch_size = 16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKHK34GG7Wt2"
      },
      "source": [
        "Define training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogk6H08V7T1g"
      },
      "source": [
        "@tf.function\r\n",
        "def train_step(X, y):\r\n",
        "    with tf.GradientTape() as tape:\r\n",
        "        pred = model(X, training=True)\r\n",
        "\r\n",
        "        loss = loss_fn(y, pred)\r\n",
        "\r\n",
        "    gradients_of_model = tape.gradient(loss, model.trainable_variables)\r\n",
        "\r\n",
        "    optimizer.apply_gradients(zip(gradients_of_model, model.trainable_variables))\r\n",
        "    acc_metric.update_state(y, pred)\r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvRu3qeJ7fD3"
      },
      "source": [
        "Define training loop\r\n",
        "\r\n",
        "❗ if you want to use sigmoid, Don't forget to change loss function and accuracy metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THzJaReg7mKe"
      },
      "source": [
        "from IPython import display\r\n",
        "\r\n",
        "for epoch in range(epochs):\r\n",
        "\r\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch + 1, epochs))\r\n",
        "\r\n",
        "    progBar = Progbar(num_training_samples, stateful_metrics=metrics_names)\r\n",
        "\r\n",
        "    # Loop on each batch of training\r\n",
        "    for idX, (X_batch, y_batch) in enumerate(trainset.batch(batch_size)):\r\n",
        "        train_loss = train_step(X_batch, y_batch)\r\n",
        "        train_acc = acc_metric.result()\r\n",
        "        acc_metric.reset_states()\r\n",
        "        values = [('train_loss', train_loss), ('train_acc', train_acc)]\r\n",
        "        progBar.update(idX * batch_size, values=values)\r\n",
        "    \r\n",
        "    # plot CAM and image for visualization on each epoch\r\n",
        "    CAMs = create_CAM(X_test, model, activation='softmax')\r\n",
        "\r\n",
        "    pred_prob = model.predict(X_test)\r\n",
        "    pred = np.argmax(pred_prob, axis=-1) # for softmax \r\n",
        "    plot_CAM(X_test, CAMs, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK9UD9uW3BhG"
      },
      "source": [
        "# Create .gif file for visualization CAM on each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_6YJpZkBWwu"
      },
      "source": [
        "import imageio\r\n",
        "anim_file = 'pretrained_visualization_CAM.gif'\r\n",
        "\r\n",
        "with imageio.get_writer(anim_file, mode='i') as writer:\r\n",
        "  for i in tqdm(range(epochs)):\r\n",
        "    filename = str(i+1) + '.png'\r\n",
        "    image = imageio.imread(filename)\r\n",
        "    writer.append_data(image)\r\n",
        "  writer.append_data(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJDPeo_3NNu"
      },
      "source": [
        "# Evaluate "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBh2b5RVT49k"
      },
      "source": [
        "\r\n",
        "_, acc = model.evaluate(X_test, y_test, verbose=1)\r\n",
        "print(\"Accuracy:\", acc*100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
