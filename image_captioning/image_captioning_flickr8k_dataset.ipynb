{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset (flickr8k)"
      ],
      "metadata": {
        "id": "yG8iFnjMhUJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFOrH1Zm036n",
        "outputId": "6fb8a397-9085-42fc-8bb7-fa8b4430e10a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m124.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "biHM_IhLbVga"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your kaggle.json API key\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "VcHc8quXbvsf",
        "outputId": "3d40a6e7-a121-437b-bd82-39d9637dad16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c251ae21-7434-41ff-a695-56b5a4b30126\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c251ae21-7434-41ff-a695-56b5a4b30126\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"mailastro\",\"key\":\"70bb5f56ab340cac467e9c97d300731c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move kaggle.json and set permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "9cLE01OHbx4d"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset directly to /content\n",
        "!kaggle datasets download -d adityajn105/flickr8k -p /content --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilP7nmUWbx9I",
        "outputId": "724d4307-686d-4663-eea8-04b7253b6872"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
            "License(s): CC0-1.0\n",
            "Downloading flickr8k.zip to /content\n",
            " 96% 0.99G/1.04G [00:05<00:00, 143MB/s]\n",
            "100% 1.04G/1.04G [00:05<00:00, 202MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We want to convert text --> numerical values\n",
        "\n",
        "1.   We need a vocabulary mapping each word to an index\n",
        "2.   We need to setup a Pytorch dataset to load the data\n",
        "3.   Setup padding of every batch (all examples should be of the same seq_len and setup dataloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "ac8XWO6rhdUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import spacy # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "-48KV1k_kdlV"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    self.freq_threshold = freq_threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "  def build_vocabulary(self, sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "\n",
        "        if frequencies[word] == self.freq_threshold:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "    return [\n",
        "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "        for token in tokenized_text\n",
        "    ]\n",
        "\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    # Get img, caption columns\n",
        "    self.imgs = self.df[\"image\"]\n",
        "    self.captions = self.df[\"caption\"]\n",
        "\n",
        "    # Initialize vocabulary and build vocab\n",
        "    self.vocab = Vocabulary(freq_threshold)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "\n",
        "    numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "    numericalized_caption += self.vocab.numericalize(caption)\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return img, torch.tensor(numericalized_caption)\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "    return imgs, targets"
      ],
      "metadata": {
        "id": "LhAOzhaDlCti"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True\n",
        "):\n",
        "  dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "  pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "  loader = DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=shuffle,\n",
        "      pin_memory=pin_memory,\n",
        "      collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "      )\n",
        "\n",
        "  return loader, dataset"
      ],
      "metadata": {
        "id": "4NrwJFkKwfUI"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construct the model"
      ],
      "metadata": {
        "id": "Wz5h2ZX9QyyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "SyNzh_b0Q3td"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "\n",
        "        # Load pretrained EfficientNet-B0\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Replace the classifier layer with a new Linear layer\n",
        "        in_features = self.efficientnet.classifier[1].in_features\n",
        "        self.efficientnet.classifier[1] = nn.Linear(in_features, embed_size)\n",
        "\n",
        "        # Freeze layers if needed\n",
        "        for name, param in self.efficientnet.named_parameters():\n",
        "            if \"classifier.1.weight\" in name or \"classifier.1.bias\" in name:\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = train_CNN\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.efficientnet(images)  # returns tensor\n",
        "        return self.dropout(self.relu(features))\n"
      ],
      "metadata": {
        "id": "3cwvnIxFXiDU"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "uI_5-DkgZXm3"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(CNNtoRNN, self).__init__()\n",
        "    self.encoderCNN = EncoderCNN(embed_size)\n",
        "    self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "    features = self.encoderCNN(images)\n",
        "    outputs = self.decoderRNN(features, captions)\n",
        "    return outputs\n",
        "\n",
        "  def caption_image(self, image, vocabulary, max_length=50):\n",
        "    result_caption = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      x = self.encoderCNN(image).unsqueeze(0)\n",
        "      states = None\n",
        "\n",
        "      for _ in range(max_length):\n",
        "        hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "        output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "        predicted = output.argmax(1)\n",
        "        result_caption.append(predicted.item())\n",
        "        x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "        if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "          break\n",
        "\n",
        "    return [vocabulary.itos[idx] for idx in result_caption]\n"
      ],
      "metadata": {
        "id": "nOWLhhxXdO3B"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "rL2yG5xrzScZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_examples(model, device, dataset):\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    test_img1 = transform(Image.open(\"test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "    print(\n",
        "        \"Example 1 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img2 = transform(\n",
        "        Image.open(\"test_examples/child.jpg\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "    print(\n",
        "        \"Example 2 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img3 = transform(Image.open(\"test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n",
        "        0\n",
        "    )\n",
        "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "    print(\n",
        "        \"Example 3 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img4 = transform(\n",
        "        Image.open(\"test_examples/boat.png\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
        "    print(\n",
        "        \"Example 4 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
        "    )\n",
        "    test_img5 = transform(\n",
        "        Image.open(\"test_examples/horse.png\").convert(\"RGB\")\n",
        "    ).unsqueeze(0)\n",
        "    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
        "    print(\n",
        "        \"Example 5 OUTPUT: \"\n",
        "        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
        "    )\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    step = checkpoint[\"step\"]\n",
        "    return step"
      ],
      "metadata": {
        "id": "-CNnISqTzUCN"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "O2NHAcZ6hwOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "  transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),  # Crop the center to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
        "                         std=(0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "\n",
        "  train_loader, dataset = get_loader(\n",
        "      root_folder=\"/content/Images\",\n",
        "      annotation_file=\"/content/captions.txt\",\n",
        "      transform=transform,\n",
        "      num_workers=2,\n",
        "  )\n",
        "\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  load_model = False\n",
        "  save_model = True\n",
        "\n",
        "  # Hyperparameters\n",
        "  embed_size = 256\n",
        "  hidden_size = 256\n",
        "  vocab_size = len(dataset.vocab)\n",
        "  num_layers = 1\n",
        "  learning_rate = 3e-4\n",
        "  num_epochs = 100\n",
        "\n",
        "  # tensorboard\n",
        "  writer = SummaryWriter(\"runs/flickr\")\n",
        "  step = 0\n",
        "\n",
        "  # initialize model, loss etc\n",
        "  model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  if load_model:\n",
        "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print_examples(model, device, dataset)\n",
        "\n",
        "    if save_model:\n",
        "      checkpoint = {\n",
        "          \"state_dict\": model.state_dict(),\n",
        "          \"optimizer\": optimizer.state_dict(),\n",
        "          \"step\": step,\n",
        "      }\n",
        "      save_checkpoint(checkpoint)\n",
        "\n",
        "    for idx, (imgs, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n",
        "      imgs = imgs.to(device)\n",
        "      captions = captions.to(device)\n",
        "\n",
        "      outputs = model(imgs, captions[:-1])\n",
        "      # (seq_len, N, vocabulary_size), (seq_len, N)\n",
        "      loss = criterion(\n",
        "          outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
        "      )\n",
        "      writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "      step += 1\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward(loss)\n",
        "      optimizer.step()"
      ],
      "metadata": {
        "id": "IBBVAeqpg4r5"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkXonRdP48iS",
        "outputId": "8d1b864a-f75a-44bd-8936-ae78e025b915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: mouths quad tags offering climbing horizon beverage attacking snowbank dirty mobile workers club punk lifts alley mats squats strange chess hats clean video bricks portrait fireworks musical arena diner surfers surfers called viz landing sells paintball bag bag bag show set monument machines bread carried give navy clothing lockers topless\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: shows sheer leaves station handle meadow wilderness above seaweed little move jet genocide catching bent clothing lockers topless drops paintball posts canvas close mountain lifting girl blows monitor big 6 teenager jackson tournament incoming mouths runner fetches kicks person blocks side carpeted helping lap park tv move side pointy sprinkler\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: croquet bagpipe posts ramps balancing amid it uphill logo paintings guarding opens using grey bicycler enclosure workers reading official incoming shop shop shop rival hit bouquet windsurfing larger motor teams attacking attacking herd herd punk pick oar motorcyclists underwater shawl trucks it performers ice wire border puppies pale silhouette table\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: collared side carpeted stares gondola oar wheelie sliding lockers parlor placed placed clothing by sidelines column stools brownish mountainside built gym hockey clown camo stares gondola mouths quad tags offering climbing horizon beverage attacking snowbank dirty mobile workers club punk lifts alley mats squats strange chess hats clean video bricks\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: snowbank dirty brownish mountainside built gym hockey clown camo stares gondola mouths runner wig fenced bridge point sliding glider family leans gentleman their attack skating mural swims good beneath column grey bicycler enclosure workers ceiling limb ice ice batter breath among stool numbered lit march opposing rival babies jacked costumes\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a man in a red shirt is standing on a <UNK> . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a man in a red shirt is standing on a <UNK> . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a red shirt is standing on a <UNK> . <EOS>\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a red shirt is standing on a <UNK> . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a man in a red shirt is standing on a <UNK> . <EOS>\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a man in a blue shirt is standing on a bench . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a blue shirt is standing on a bench . <EOS>\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a blue shirt is standing on a bench . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a man in a blue shirt is standing on a bench . <EOS>\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a boy in a red shirt is standing on a bench . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a blue shirt and a black shirt and a black shirt and a woman in a white shirt and a black hat and a woman in a white shirt and a black hat and a woman in a white shirt and a black hat and a\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a blue shirt is standing on a bench . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a man in a black shirt and a black shirt and a black shirt and a woman in a blue shirt and a white shirt and a black hat and a woman in a white shirt and a black hat and a woman in a white shirt and a\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a little girl in a blue shirt is playing with a ball in the grass . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a red shirt and a black hat and a woman in a black shirt and a black hat and a woman in a white shirt and a black hat and a woman in a black shirt and a black hat and a black hat and a\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a blue shirt is riding a bike . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a man in a black shirt and a black hat and a black hat and a woman in a black jacket and a black hat and a black hat and a woman in a black jacket and a black hat and a black hat and a woman in a\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a young boy in a blue shirt is playing with a ball in a pool . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a black shirt and a hat and a woman in a black jacket and a black hat and a black hat and a black hat and a woman in a black jacket and black pants is standing on a bench . <EOS>\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a blue shirt is riding a bicycle . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a man and a woman are sitting on a bench . <EOS>\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1 CORRECT: Dog on a beach by the ocean\n",
            "Example 1 OUTPUT: <SOS> a dog is running through the grass . <EOS>\n",
            "Example 2 CORRECT: Child holding red frisbee outdoors\n",
            "Example 2 OUTPUT: <SOS> a young boy in a red shirt and a white shirt and a blue shirt and a red shirt is playing with a ball in a field . <EOS>\n",
            "Example 3 CORRECT: Bus driving by parked cars\n",
            "Example 3 OUTPUT: <SOS> a man in a white shirt and a hat is standing on a bench . <EOS>\n",
            "Example 4 CORRECT: A small boat in the ocean\n",
            "Example 4 OUTPUT: <SOS> a man in a blue shirt is riding a bike on a dirt path . <EOS>\n",
            "Example 5 CORRECT: A cowboy riding a horse in the desert\n",
            "Example 5 OUTPUT: <SOS> a man and a woman are sitting on a bench . <EOS>\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 221/1265 [00:43<03:03,  5.68it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ASxQhR38IPLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}